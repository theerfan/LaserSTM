{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from train_predict_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LSTMModel_1(nn.Module):\n",
    "    # basic one with two linear layers and final output with sigmoid\n",
    "    def __init__(\n",
    "        self, input_size, lstm_hidden_size=1024, linear_layer_size=4096, num_layers=1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = lstm_hidden_size\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size,\n",
    "            lstm_hidden_size,\n",
    "            batch_first=True,\n",
    "            dropout=0,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.fc1 = nn.Linear(lstm_hidden_size, linear_layer_size)\n",
    "        self.fc2 = nn.Linear(linear_layer_size, linear_layer_size)\n",
    "        self.fc3 = nn.Linear(linear_layer_size, input_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # hidden state\n",
    "        h_0 = torch.zeros(self.num_layers * 1, x.size(0), self.hidden_size).to(\n",
    "            x.device\n",
    "        )  # Modified line\n",
    "        # cell state\n",
    "        c_0 = torch.zeros(self.num_layers * 1, x.size(0), self.hidden_size).to(\n",
    "            x.device\n",
    "        )  # Modified line\n",
    "\n",
    "        out, (hn, cn) = self.lstm(x, (h_0, c_0))\n",
    "        out = self.fc1(out[:, -1, :])\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = torch.sigmoid(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = \"../processed_dataset/\"\n",
    "train_dataset = CustomSequence(data_dir, range(2), file_batch_size=1, model_batch_size=512)\n",
    "val_dataset = CustomSequence(data_dir, [1], file_batch_size=1, model_batch_size=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1892 * 4 + 348 * 2\n",
    "model = LSTMModel_1(input_size=8264)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 10\n",
      "Epoch 1: Train Loss=0.145181387662887573, Val Loss=0.057540912181138992\n",
      "Epoch 2 of 10\n",
      "Epoch 2: Train Loss=0.056456286460161209, Val Loss=0.044879958033561707\n",
      "Epoch 3 of 10\n",
      "Epoch 3: Train Loss=0.039471747353672981, Val Loss=0.026325631886720657\n",
      "Epoch 4 of 10\n",
      "Epoch 4: Train Loss=0.021488886326551437, Val Loss=0.014857287518680096\n",
      "Epoch 5 of 10\n",
      "Epoch 5: Train Loss=0.011351361870765686, Val Loss=0.013296050019562244\n",
      "Epoch 6 of 10\n",
      "Epoch 6: Train Loss=0.010871140751987696, Val Loss=0.012453669682145119\n",
      "Epoch 7 of 10\n",
      "Epoch 7: Train Loss=0.010995287448167801, Val Loss=0.011442775838077068\n",
      "Epoch 8 of 10\n",
      "Epoch 8: Train Loss=0.010695511475205421, Val Loss=0.009264889173209667\n",
      "Epoch 9 of 10\n",
      "Epoch 9: Train Loss=0.009273342788219452, Val Loss=0.007836075499653816\n",
      "Epoch 10 of 10\n",
      "Epoch 10: Train Loss=0.008656975813210011, Val Loss=0.007580926176160574\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LSTMModel_1(\n",
       "   (lstm): LSTM(8264, 1024, batch_first=True)\n",
       "   (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "   (fc2): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "   (fc3): Linear(in_features=4096, out_features=8264, bias=True)\n",
       "   (relu): ReLU()\n",
       " ),\n",
       " [0.14518138766288757,\n",
       "  0.05645628646016121,\n",
       "  0.03947174735367298,\n",
       "  0.021488886326551437,\n",
       "  0.011351361870765686,\n",
       "  0.010871140751987696,\n",
       "  0.010995287448167801,\n",
       "  0.010695511475205421,\n",
       "  0.009273342788219452,\n",
       "  0.00865697581321001],\n",
       " [0.05754091218113899,\n",
       "  0.04487995803356171,\n",
       "  0.026325631886720657,\n",
       "  0.014857287518680096,\n",
       "  0.013296050019562244,\n",
       "  0.012453669682145119,\n",
       "  0.011442775838077068,\n",
       "  0.009264889173209667,\n",
       "  0.007836075499653816,\n",
       "  0.007580926176160574])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(model, train_dataset, num_epochs=10, val_dataset=val_dataset, use_gpu=True, data_parallel=True, out_dir=\".\", model_name=\"model\", verbose=1, save_checkpoints=True, custom_loss=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = CustomSequence(data_dir, [2], file_batch_size=1, model_batch_size=512, test_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting batch 1/2\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../processed_dataset/X_new_90.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\TheRe\\Desktop\\Summer-Project\\NeurIPS2023\\LSTM\\training_predicting.ipynb Cell 9\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/TheRe/Desktop/Summer-Project/NeurIPS2023/LSTM/training_predicting.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m predict(model, model_param_path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodel_epoch_2.pth\u001b[39m\u001b[39m\"\u001b[39m, test_dataset\u001b[39m=\u001b[39mtest_dataset, use_gpu\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, data_parallel\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, output_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, output_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mall_preds.npy\u001b[39m\u001b[39m\"\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\TheRe\\Desktop\\Summer-Project\\NeurIPS2023\\LSTM\\train_predict_utils.py:232\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(model, model_param_path, test_dataset, use_gpu, data_parallel, output_dir, output_name, verbose)\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPredicting batch \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(test_dataset)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    231\u001b[0m sample_generator \u001b[39m=\u001b[39m test_dataset[i]\n\u001b[1;32m--> 232\u001b[0m \u001b[39mfor\u001b[39;00m X, y \u001b[39min\u001b[39;00m sample_generator:\n\u001b[0;32m    233\u001b[0m     X, y \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32), y\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m    234\u001b[0m     X, y \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(device), y\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\TheRe\\Desktop\\Summer-Project\\NeurIPS2023\\LSTM\\train_predict_utils.py:61\u001b[0m, in \u001b[0;36mCustomSequence.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(batch_x, batch_y):\n\u001b[0;32m     59\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_mode:\n\u001b[0;32m     60\u001b[0m         \u001b[39m# Every 100th sample\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m         temp_x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_dir, x))[::\u001b[39m100\u001b[39m]\n\u001b[0;32m     62\u001b[0m         \u001b[39m# Every 100th sample, starting from 100th sample\u001b[39;00m\n\u001b[0;32m     63\u001b[0m         temp_y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_dir, y))[\u001b[39m99\u001b[39m:][::\u001b[39m100\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\TheRe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\npyio.py:390\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    388\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    389\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 390\u001b[0m     fid \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39menter_context(\u001b[39mopen\u001b[39;49m(os_fspath(file), \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m    391\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    393\u001b[0m \u001b[39m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../processed_dataset/X_new_90.npy'"
     ]
    }
   ],
   "source": [
    "predict(model, model_param_path=\"model_epoch_2.pth\", test_dataset=test_dataset, use_gpu=True, data_parallel=False, output_dir=\".\", output_name=\"all_preds.npy\", verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "15a3a1d02b16c5f0f7818f79310520e3bf618f6f01b420b9dd242659c73da7ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
